{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotary Positional Embedding and Helper Functions\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"Implements Rotary Positional Embedding (RoPE)\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "    \n",
    "    def forward(self, seq_len, device):\n",
    "        \"\"\"\n",
    "        Generates sinusoidal embeddings for RoPE.\n",
    "\n",
    "        Args:\n",
    "            seq_len (int): The length of the sequence.\n",
    "            device (torch.device): The device to create tensors on.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Sinusoidal embeddings of shape (seq_len, dim).\n",
    "        \"\"\"\n",
    "        t = torch.arange(seq_len, device=device).type_as(self.inv_freq)\n",
    "        freqs = torch.einsum('i , j -> i j', t, self.inv_freq)\n",
    "        # Combine sin and cos embeddings\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        return emb  # (seq_len, dim)\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Helper function for rotating half of the dimensions\"\"\"\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, sinusoidal_pos):\n",
    "    \"\"\"\n",
    "    Applies RoPE to query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (torch.Tensor): Query tensor of shape (batch_size, num_heads, seq_len, head_dim).\n",
    "        k (torch.Tensor): Key tensor of shape (batch_size, num_heads, seq_len, head_dim).\n",
    "        sinusoidal_pos (torch.Tensor): Sinusoidal embeddings of shape (seq_len, head_dim).\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: Rotated query and key tensors.\n",
    "    \"\"\"\n",
    "    sin, cos = sinusoidal_pos.sin(), sinusoidal_pos.cos()\n",
    "    sin = sin.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim)\n",
    "    cos = cos.unsqueeze(0).unsqueeze(0)\n",
    "    q_rot = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_rot = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_rot, k_rot\n",
    "\n",
    "# Neural network components\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    \"\"\"Simple feed-forward network to project input features\"\"\"\n",
    "    def __init__(self, d_in, d_model):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Linear(d_in, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention mechanism with RoPE\"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        assert self.head_dim * num_heads == d_model, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.qkv_proj = nn.Linear(d_model, 3 * d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        self.rotary_emb = RotaryEmbedding(self.head_dim)\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the multi-head attention with RoPE.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (seq_len, batch_size, d_model).\n",
    "            key_padding_mask (torch.Tensor, optional): Padding mask of shape (batch_size, seq_len).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (seq_len, batch_size, d_model).\n",
    "        \"\"\"\n",
    "        seq_len, batch_size, _ = x.size()\n",
    "\n",
    "        # Project to query, key, value\n",
    "        qkv = self.qkv_proj(x)  # Shape: (seq_len, batch_size, 3*d_model)\n",
    "        qkv = qkv.view(seq_len, batch_size, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 1, 3, 0, 4)  # Shape: (3, batch_size, num_heads, seq_len, head_dim)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # Each shape: (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "        # Generate sinusoidal positional embeddings\n",
    "        sinusoidal_pos = self.rotary_emb(seq_len, device=x.device)  # Shape: (seq_len, head_dim)\n",
    "\n",
    "        # Apply RoPE to q and k\n",
    "        q, k = apply_rotary_pos_emb(q, k, sinusoidal_pos)\n",
    "\n",
    "        # Compute scaled dot-product attention\n",
    "        attn_scores = torch.einsum('bnqd, bnkd -> bnqk', q, k)  # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        attn_scores = attn_scores / (self.head_dim ** 0.5)\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            # key_padding_mask: (batch_size, seq_len)\n",
    "            attn_scores = attn_scores.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2), float('-inf'))\n",
    "\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)  # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "        attn_output = torch.einsum('bnqk, bnkd -> bnqd', attn_probs, v)  # Shape: (batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "        # Concatenate heads\n",
    "        attn_output = attn_output.permute(2, 0, 1, 3).contiguous()  # Shape: (seq_len, batch_size, num_heads, head_dim)\n",
    "        attn_output = attn_output.view(seq_len, batch_size, self.d_model)  # Shape: (seq_len, batch_size, d_model)\n",
    "\n",
    "        # Final linear projection\n",
    "        output = self.out_proj(attn_output)  # Shape: (seq_len, batch_size, d_model)\n",
    "\n",
    "        return output\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Transformer encoder layer with self-attention and feed-forward network\"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (seq_len, batch_size, d_model).\n",
    "            src_key_padding_mask (torch.Tensor, optional): Padding mask.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of the same shape as input.\n",
    "        \"\"\"\n",
    "        # Multi-head attention with residual connection and layer normalization\n",
    "        attn_output = self.mha(x, key_padding_mask=src_key_padding_mask)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        # Feed-forward network with residual connection and layer normalization\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + ff_output)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Stack of transformer encoder layers\"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        Passes the input through the stack of encoder layers.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (seq_len, batch_size, d_model).\n",
    "            src_key_padding_mask (torch.Tensor, optional): Padding mask.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (seq_len, batch_size, d_model).\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_key_padding_mask)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Simple linear decoder to project back to output dimension\"\"\"\n",
    "    def __init__(self, d_model, d_out):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Projects the encoder output to the desired output dimension.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (seq_len, batch_size, d_model).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (seq_len, batch_size, d_out).\n",
    "        \"\"\"\n",
    "        return self.linear(x)\n",
    "\n",
    "class Astromer(nn.Module):\n",
    "    \"\"\"Main model architecture for light curve processing\"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.fnn = FeedForwardNetwork(1, d_model)\n",
    "        self.encoder = Encoder(d_model, num_heads, d_ff, num_layers)\n",
    "        self.decoder = Decoder(d_model, 1)\n",
    "\n",
    "    def forward(self, times, magnitudes, lengths, mask_prob=0.15):\n",
    "        \"\"\"\n",
    "        Processes the input light curves and reconstructs the magnitudes.\n",
    "\n",
    "        Args:\n",
    "            times (torch.Tensor): Time steps tensor of shape (batch_size, seq_len).\n",
    "            magnitudes (torch.Tensor): Magnitudes tensor of shape (batch_size, seq_len).\n",
    "            lengths (torch.Tensor): Actual lengths of each sequence in the batch.\n",
    "            mask_prob (float): Probability of masking an input magnitude.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Reconstructed magnitudes of shape (batch_size, seq_len).\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = magnitudes.size()\n",
    "\n",
    "        # Normalize time steps\n",
    "        times = self.normalize_times(times, lengths)\n",
    "\n",
    "        # Apply masking and initial feature projection\n",
    "        masked_magnitudes = self.mask_magnitudes(magnitudes, lengths, mask_prob)\n",
    "        x = self.fnn(masked_magnitudes.unsqueeze(-1))  # Shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Prepare input for transformer: transpose to (seq_len, batch_size, d_model)\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        # Create attention padding mask\n",
    "        padding_mask = self.create_padding_mask(lengths, seq_len)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "        # Process through encoder\n",
    "        encoded = self.encoder(x, src_key_padding_mask=padding_mask)\n",
    "\n",
    "        # Decode to final predictions\n",
    "        decoded = self.decoder(encoded)  # Shape: (seq_len, batch_size, 1)\n",
    "        reconstructed = decoded.squeeze(-1).transpose(0, 1)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "        return reconstructed\n",
    "\n",
    "    def normalize_times(self, times, lengths):\n",
    "        \"\"\"\n",
    "        Normalizes time steps within each sequence to the range [0, 1].\n",
    "\n",
    "        Args:\n",
    "            times (torch.Tensor): Time steps tensor of shape (batch_size, seq_len).\n",
    "            lengths (torch.Tensor): Actual lengths of each sequence.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Normalized time steps tensor.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = times.size()\n",
    "        times_normalized = times.clone()\n",
    "        for i in range(batch_size):\n",
    "            length = lengths[i]\n",
    "            seq_times = times[i, :length]\n",
    "            min_t = seq_times.min()\n",
    "            max_t = seq_times.max()\n",
    "            if max_t > min_t:\n",
    "                times_normalized[i, :length] = (seq_times - min_t) / (max_t - min_t)\n",
    "            else:\n",
    "                times_normalized[i, :length] = 0.0  # All times are the same\n",
    "        return times_normalized\n",
    "\n",
    "    def mask_magnitudes(self, magnitudes, lengths, mask_prob):\n",
    "        \"\"\"\n",
    "        Randomly masks input magnitudes for denoising training.\n",
    "\n",
    "        Args:\n",
    "            magnitudes (torch.Tensor): Magnitudes tensor of shape (batch_size, seq_len).\n",
    "            lengths (torch.Tensor): Actual lengths of each sequence.\n",
    "            mask_prob (float): Probability of masking an input magnitude.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Masked magnitudes tensor.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = magnitudes.size()\n",
    "        # Create a mask for valid positions\n",
    "        valid_mask = torch.arange(seq_len, device=lengths.device).unsqueeze(0) < lengths.unsqueeze(1)\n",
    "        # Create random mask for masking magnitudes\n",
    "        random_mask = (torch.rand(batch_size, seq_len, device=magnitudes.device) < mask_prob) & valid_mask\n",
    "        masked_magnitudes = magnitudes.clone()\n",
    "        masked_magnitudes[random_mask] = 0.0\n",
    "        return masked_magnitudes\n",
    "\n",
    "    def create_padding_mask(self, lengths, max_length):\n",
    "        \"\"\"\n",
    "        Creates mask for padding tokens in attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            lengths (torch.Tensor): Actual lengths of each sequence.\n",
    "            max_length (int): Maximum sequence length in the batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Padding mask of shape (batch_size, seq_len).\n",
    "        \"\"\"\n",
    "        # Mask positions that are padding (True for padding positions)\n",
    "        padding_mask = torch.arange(max_length, device=lengths.device).unsqueeze(0) >= lengths.unsqueeze(1)\n",
    "        return padding_mask  # Shape: (batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class setup\n",
    "class LightCurveDataset(Dataset):\n",
    "    \"\"\"Dataset class for light curve data with padding\"\"\"\n",
    "    def __init__(self, times, magnitudes, max_length):\n",
    "        self.times = times\n",
    "        self.magnitudes = magnitudes\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.times)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        time = self.times[idx]\n",
    "        magnitude = self.magnitudes[idx]\n",
    "        length = len(time)\n",
    "\n",
    "        # Pad sequences to max_length\n",
    "        padded_time = torch.zeros(self.max_length)\n",
    "        padded_magnitude = torch.zeros(self.max_length)\n",
    "        \n",
    "        padded_time[:length] = torch.tensor(time, dtype=torch.float32)\n",
    "        padded_magnitude[:length] = torch.tensor(magnitude, dtype=torch.float32)\n",
    "        \n",
    "        return padded_time, padded_magnitude, length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "d_ff = 256\n",
    "num_layers = 3\n",
    "lmax = 100\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = Astromer(\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ff=d_ff,\n",
    "    num_layers=num_layers\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Astromer(\n",
       "  (fnn): FeedForwardNetwork(\n",
       "    (mlp): Linear(in_features=1, out_features=128, bias=True)\n",
       "  )\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x EncoderLayer(\n",
       "        (mha): MultiHeadAttention(\n",
       "          (qkv_proj): Linear(in_features=128, out_features=384, bias=True)\n",
       "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (linear): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "file_path = '../Data/synthetic_light_curves.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "grouped = df.groupby('sample_id')\n",
    "\n",
    "# Extract time series data\n",
    "times = [group['time_mjd'].values for _, group in grouped]\n",
    "magnitudes = [group['magnitude'].values for _, group in grouped]\n",
    "max_length = max(len(t) for t in times)\n",
    "\n",
    "# Split into train and test sets\n",
    "train_times, test_times, train_mags, test_mags = train_test_split(\n",
    "    times, magnitudes, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = LightCurveDataset(train_times, train_mags, max_length)\n",
    "test_dataset = LightCurveDataset(test_times, test_mags, max_length)\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, optimizer, train_loader, test_loader, num_epochs=10):\n",
    "    \"\"\"Training loop with validation\"\"\"\n",
    "    criterion = nn.MSELoss(reduction='none')\n",
    "    best_test_loss = float('inf')\n",
    "    model_dir = '../Models'\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    best_model_path = os.path.join(model_dir, 'RoPE-redo-astromer.pth')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for times, magnitudes, lengths in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = model(times, magnitudes, lengths)\n",
    "            loss = criterion(reconstructed, magnitudes)\n",
    "            # Apply mask to compute loss only on valid timesteps\n",
    "            mask = torch.arange(magnitudes.size(1)).expand(magnitudes.size(0), magnitudes.size(1)) < lengths.unsqueeze(1)\n",
    "            loss = (loss * mask.float()).sum() / mask.sum()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for times, magnitudes, lengths in test_loader:\n",
    "                reconstructed = model(times, magnitudes, lengths)\n",
    "                loss = criterion(reconstructed, magnitudes)\n",
    "                mask = torch.arange(magnitudes.size(1)).expand(magnitudes.size(0), magnitudes.size(1)) < lengths.unsqueeze(1)\n",
    "                loss = (loss * mask.float()).sum() / mask.sum()\n",
    "                test_loss += loss.item()\n",
    "        \n",
    "        test_loss /= len(test_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"New best model saved to {best_model_path}\")\n",
    "    \n",
    "    print(f\"Training completed. Best model saved with test loss: {best_test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5378, Test Loss: 0.1391\n",
      "New best model saved to ../Models/RoPE-redo-astromer.pth\n",
      "Epoch 2/10, Train Loss: 0.0904, Test Loss: 0.0790\n",
      "New best model saved to ../Models/RoPE-redo-astromer.pth\n",
      "Epoch 3/10, Train Loss: 0.0781, Test Loss: 0.0776\n",
      "New best model saved to ../Models/RoPE-redo-astromer.pth\n",
      "Epoch 4/10, Train Loss: 0.0737, Test Loss: 0.0713\n",
      "New best model saved to ../Models/RoPE-redo-astromer.pth\n",
      "Epoch 5/10, Train Loss: 0.0721, Test Loss: 0.0694\n",
      "New best model saved to ../Models/RoPE-redo-astromer.pth\n",
      "Epoch 6/10, Train Loss: 0.0652, Test Loss: 0.0615\n",
      "New best model saved to ../Models/RoPE-redo-astromer.pth\n",
      "Epoch 7/10, Train Loss: 0.0570, Test Loss: 0.0532\n",
      "New best model saved to ../Models/RoPE-redo-astromer.pth\n",
      "Epoch 8/10, Train Loss: 0.0553, Test Loss: 0.0528\n",
      "New best model saved to ../Models/RoPE-redo-astromer.pth\n",
      "Epoch 9/10, Train Loss: 0.0528, Test Loss: 0.0496\n",
      "New best model saved to ../Models/RoPE-redo-astromer.pth\n",
      "Epoch 10/10, Train Loss: 0.0501, Test Loss: 0.0459\n",
      "New best model saved to ../Models/RoPE-redo-astromer.pth\n",
      "Training completed. Best model saved with test loss: 0.0459\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "train(model, optimizer, train_loader, test_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time per sample: 20.27 ms\n",
      "95.0% CI: [5.51, 29.91] ms\n"
     ]
    }
   ],
   "source": [
    "# Test inference time\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def test_inference_time(model, test_loader, num_runs=100, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Measure the average inference time per sample for a given model.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model to test.\n",
    "        test_loader: DataLoader containing the test dataset.\n",
    "        num_runs (int): Number of inference runs to average over (default: 100).\n",
    "        confidence_level (float): Confidence level for interval calculation (default: 0.95).\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    # Get the device (CPU or GPU) that the model is on\n",
    "    device = next(model.parameters()).device\n",
    "    total_time = 0\n",
    "    total_samples = 0\n",
    "    all_times = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        for times, magnitudes, lengths in test_loader:\n",
    "            # Move input data to the same device as the model\n",
    "            times, magnitudes, lengths = times.to(device), magnitudes.to(device), lengths.to(device)\n",
    "            batch_size = times.size(0)\n",
    "            \n",
    "            # Perform a warm-up run to ensure GPU is ready\n",
    "            _ = model(times, magnitudes, lengths)\n",
    "            \n",
    "            # Timed runs\n",
    "            batch_times = []\n",
    "            for _ in range(num_runs):\n",
    "                start_time = time.time()\n",
    "                _ = model(times, magnitudes, lengths)\n",
    "                end_time = time.time()\n",
    "                batch_times.extend([end_time - start_time] * batch_size)\n",
    "            \n",
    "            all_times.extend(batch_times)\n",
    "            total_time += sum(batch_times)\n",
    "            total_samples += batch_size * num_runs\n",
    "    \n",
    "    # Calculate overall average inference time per sample\n",
    "    avg_inference_time = total_time / total_samples\n",
    "    \n",
    "    # Calculate confidence interval\n",
    "    alpha = 1 - confidence_level\n",
    "    times_array = np.array(all_times)\n",
    "    ci_lower = np.percentile(times_array, alpha * 100 / 2)\n",
    "    ci_upper = np.percentile(times_array, 100 - (alpha * 100 / 2))\n",
    "    \n",
    "    print(f\"Average inference time per sample: {avg_inference_time*1000:.2f} ms\")\n",
    "    print(f\"{confidence_level*100:.1f}% CI: [{ci_lower*1000:.2f}, {ci_upper*1000:.2f}] ms\")\n",
    "\n",
    "# Test inference time using the trained model and test data loader\n",
    "test_inference_time(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvements: \n",
    "Both Model 1 (@02_RoPE.ipynb) and Model 2 (@03_RoPE-redo.ipynb) implement a transformer-based architecture with Rotary Positional Encoding (RoPE) for processing light curve data. Here is a summary of their key differences and insights:\n",
    "\n",
    "**Rotary Positional Encoding (RoPE) Implementation:**\n",
    "- Model 1 uses a `RoPEEncoding` class to create sinusoidal position encodings for time steps, combining sine and cosine values along the last dimension and adding them to input embeddings.\n",
    "- Model 2 employs a `RotaryEmbedding` class for more granular positional embeddings, focusing on half-rotation and applying it in `apply_rotary_pos_emb`. This method rotates q and k tensors separately with sinusoidal embeddings, closely aligning with RoPE as described in research literature.\n",
    "- **Evaluation:** Model 2's approach aligns better with standard RoPE, applying the rotary transformation at each attention head level, which should improve temporal representations, especially for tasks sensitive to time ordering, like light curve analysis.\n",
    "\n",
    "**Multi-Head Attention Module:**\n",
    "- Model 1 uses PyTorch’s `nn.MultiheadAttention` without explicitly modifying attention weights with RoPE.\n",
    "- Model 2 customizes the `MultiHeadAttention` class, projecting qkv (queries, keys, and values) and applying RoPE directly to q and k before computing attention scores. This provides a detailed representation that aligns with positional information, potentially enhancing temporal attention modeling.\n",
    "- **Evaluation:** Model 2’s custom attention mechanism with direct RoPE application is more detailed and should offer better handling of temporal data in sequences. It’s a more flexible and modular design.\n",
    "\n",
    "**Encoding Layer Normalization:**\n",
    "Both models use Layer Normalization after attention and feed-forward steps. Model 1 has a simpler architecture, while Model 2 emphasizes residual connections by making the `MultiHeadAttention` module modular.\n",
    "\n",
    "**Time Normalization in Encoder:**\n",
    "- Model 1 does not explicitly normalize time steps in the forward function.\n",
    "- Model 2 normalizes time steps within each sequence to a [0,1] range, which could improve model stability and interpretability, particularly when working with time series data that may vary widely in scale.\n",
    "- **Evaluation:** Model 2’s time normalization is beneficial for handling irregular time steps, common in observational data like light curves.\n",
    "\n",
    "**Flexibility in Padding and Masking:**\n",
    "- Model 1 implements custom methods for masking magnitudes and creating padding masks, but its padding mask application is less direct.\n",
    "- Model 2 provides robust handling of padding with custom masking for attention and magnitudes, improving the model's ability to focus on relevant time steps in irregularly padded sequences.\n",
    "- **Evaluation:** Model 2's masking approach is more robust for datasets with irregular or missing observations, making it more effective in noisy datasets.\n",
    "\n",
    "**Application Suitability and Efficiency:**\n",
    "- Model 1 is simpler and easier to implement, which could make it faster for prototyping but may lack temporal precision for complex tasks.\n",
    "- Model 2 is more detailed and adheres closely to RoPE theory, making it potentially better for time-sensitive applications with longer or variable-length sequences.\n",
    "\n",
    "# **Recommendation:**\n",
    "Model 2 is more effective for time-sensitive data processing due to its refined application of RoPE in attention, time normalization, and masking. This approach ensures that temporal information is integrated at a more granular level, enhancing interpretability and performance in tasks requiring high sensitivity to temporal patterns. However, if quick prototyping is preferred over temporal accuracy, Model 1 offers a simpler implementation with fewer dependencies on custom modules."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
